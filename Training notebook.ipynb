{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dfkEbmkfpLZ"
      },
      "source": [
        "# Siamese network for detecting misprinted photos\n",
        "## Loading the data\n",
        "### The data has two folders. One folder has normal photos. The other has misprinted photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:25:24.533715Z",
          "iopub.status.busy": "2024-02-07T11:25:24.533392Z",
          "iopub.status.idle": "2024-02-07T11:25:38.376345Z",
          "shell.execute_reply": "2024-02-07T11:25:38.375364Z",
          "shell.execute_reply.started": "2024-02-07T11:25:24.533663Z"
        },
        "id": "PmYCrDq8XpVQ",
        "outputId": "6a9983ce-2c27-434d-b0e7-88677966c737",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-5.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:25:38.37863Z",
          "iopub.status.busy": "2024-02-07T11:25:38.37797Z",
          "iopub.status.idle": "2024-02-07T11:25:49.89022Z",
          "shell.execute_reply": "2024-02-07T11:25:49.889151Z",
          "shell.execute_reply.started": "2024-02-07T11:25:38.37859Z"
        },
        "id": "kdJBum3hXpVT",
        "outputId": "349eabf7-a25c-4345-dab0-cd4744dbf26c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VMottI0IyzCloyLXL7uIRWPTJj7atAFf\n",
            "From (redirected): https://drive.google.com/uc?id=1VMottI0IyzCloyLXL7uIRWPTJj7atAFf&confirm=t&uuid=d9fc736d-11d6-47bb-ac59-270e1bbe1cee\n",
            "To: /kaggle/working/text3.zip\n",
            "100%|███████████████████████████████████████| 1.03G/1.03G [00:09<00:00, 112MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1VMottI0IyzCloyLXL7uIRWPTJj7atAFf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-02-07T11:25:49.892813Z",
          "iopub.status.busy": "2024-02-07T11:25:49.892446Z",
          "iopub.status.idle": "2024-02-07T11:26:11.088163Z",
          "shell.execute_reply": "2024-02-07T11:26:11.08667Z",
          "shell.execute_reply.started": "2024-02-07T11:25:49.892779Z"
        },
        "id": "orBBnyScXpVU",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "f370ad49-3e07-4e27-e7f1-5daf75609509",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  text3.zip\n",
            "   creating: Text 3 images/\n",
            "   creating: Text 3 images/Text 3 - Training images/\n",
            "   creating: Text 3 images/Text 3 - Training images/Pass/\n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/01 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/02 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/02.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/03.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/03 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/04 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/04.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131444480.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131444480 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131439667.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131439667 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131435133.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131435133 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131322372.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131322372 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131313762 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131313762.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131309055 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131309055.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131304111 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131304111.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131211038 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131211038.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131207116.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131207116 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131159572.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131159572 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131154316 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131154316.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131025840 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131025840.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131013298 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131013298.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131008093.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120131008093 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120130815003.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120130815003 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120130753447.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120130753447 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135426247.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135426247 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/05 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/05.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/06 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/06.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/07.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/07 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/08.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/08 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/09 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/09.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135422125.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135422125 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/10.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/10 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/11.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/11 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/12 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/12.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/13 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/13.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/14 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/14.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135414532 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135414532.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/15.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/15 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/16.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/16 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/17.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/17 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135332597.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135332597 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/18 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/18.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/19.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/19 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/20.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/20 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/21.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/21 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/22.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/22 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135328697 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135328697.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/23.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/23 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/24.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/24 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/25.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/25 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/26.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/26 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135322404 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135322404.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/27 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/27.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/28 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/28.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/29.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/29 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/30.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/30 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135226224.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135226224 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/31.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/31 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/32 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/32.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/33.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/33 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135218914.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135218914 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/34 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/34.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/35.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/35 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/36.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/36 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/37 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/37.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135036635 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135036635.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/38.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/38 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/39.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/39 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/40 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/40.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/41 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/41.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134023851 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134023851.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/42.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/42 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/43.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/43 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/44.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/44 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134020401 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134020401.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/45.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/45 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/46 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/46.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/47.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/47 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/48.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/48 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/49 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/49.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134017283 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120134017283.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/50 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/50.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/51 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/51.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/52 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/52.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/53.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/53 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/54 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/54.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/55 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/55.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/56 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/56.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133942698.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133942698 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/57 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/57.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/58.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/58 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/59.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/59 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133939090.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133939090 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/60 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/60.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/612.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/612 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/62.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/62 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133933069.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133933069 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/61 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/61.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/63 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/63.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/64.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/64 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/65 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/65.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133847980 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133847980.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/66.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/66 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/67 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/67.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133844605 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133844605.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/68.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/68 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/69 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/69.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/70 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/70.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/71 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/71.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/72.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/72 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133656811.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133656811 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/73.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/73 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/74 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/74.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133648461.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133648461 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/75 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/75.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133643083 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133643083.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/76.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/76 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/77 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/77.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/78 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/78.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135430529.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120135430529 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/80 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/80.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/81.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/81 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133553933.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133553933 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/82.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/82 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/83.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/83 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133551145 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133551145.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/84 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/84.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/85 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/85.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133544716.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133544716 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/86.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/86 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/87 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/87.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/88.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/88 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133539949 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133539949.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/89 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/89.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133502830 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133502830.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/90 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/90.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/91 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/91.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133459562.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133459562 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/92.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/92 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/93.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/93 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133413674 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133413674.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/94.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/94 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/95 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/95.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133410447 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133410447.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/96.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/96 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/97.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/97 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133406476 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133406476.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/98.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/98 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133402182 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133402182.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/99.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/99 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133321109 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133321109.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/100 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/100.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133316666 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133316666.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133312620.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133312620 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/101 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/101.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133231702.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133231702 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133224515.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133224515 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133218665.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133218665 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133213802.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133213802 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133117484 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133117484.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133112541 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133112541.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133106917 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133106917.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133103767 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133103767.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133007842 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120133007842.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132954060.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132954060 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132949791 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132949791.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132944718.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132854803 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132854803.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132849818 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132849818.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132845807 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Image_20240120132845807.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131444480 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131309055 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 04 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131008093.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131313762.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131008093 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131013298.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131309055.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 03 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120131439667.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 08 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120134023851.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 64.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 63.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 42.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 09 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of Image_20240120133551145 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of 83 - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Pass/Copy of negative.bmp  \n",
            "   creating: Text 3 images/Text 3 - Training images/Missing Text/\n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/a.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/b.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/c.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/d.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/e.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/f.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/g.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/h.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (13).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (10).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (8).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (15).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (9).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (3).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (14).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (6).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (2).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (12).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (4).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (16).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (7).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (11).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/i_small missing - Copy (5).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 3 letter.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 3 letter 01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 3 letter 02.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 1 letter 01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 1 letter 02.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 2 letter 01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Miss 3 letter 03.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/j.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/k.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/02.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/03.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/04.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/05.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/06.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/07.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/08.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/09.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/10.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/11.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/12.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/13.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/14.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/15.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/16.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/17.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/18.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/19.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/20.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/21.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/22.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/23.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/24.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/25.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/26.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/27.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/28.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/29.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/30.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/31.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/32.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/33.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/34.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/35.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/36.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/37.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/38.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/39.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/40.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/41.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/42.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/43.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/44.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/48.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/49.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/50.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/51.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/52.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/53.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/54.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/55 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/56.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/57.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/58 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/59.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/60.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/61 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/62.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/63.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/64 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/65.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/66.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/68.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/69.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/70 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/71.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/73 (1).bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/74.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Image 01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/75.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/76.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/77.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/78.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/79.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/80.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Image_20240120133112541.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/81.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/82.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/83.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Image_20240120133106917.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/84.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/85.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/86.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/87.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/88.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/89.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/90.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/91.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/92.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/93.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/94.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/95.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/97.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/98.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/99.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/100.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/Image_20240120133103767.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 01.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 02.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 03.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 04.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 05.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 06.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 07.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 08.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 09.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 10.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/missing 11.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/positive.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/55.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/58.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/61.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/64.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/67.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/70.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/73.bmp  \n",
            "  inflating: Text 3 images/Text 3 - Training images/Missing Text/76_.bmp  \n"
          ]
        }
      ],
      "source": [
        "!unzip text3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3YyyIkgfpLg"
      },
      "source": [
        "# Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:26:15.874722Z",
          "iopub.status.busy": "2024-02-07T11:26:15.874317Z",
          "iopub.status.idle": "2024-02-07T11:26:29.536772Z",
          "shell.execute_reply": "2024-02-07T11:26:29.535752Z",
          "shell.execute_reply.started": "2024-02-07T11:26:15.874669Z"
        },
        "id": "SYD33WefgZlM",
        "outputId": "03723f01-3159-457d-928c-3b8feddf3ae4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-07 11:26:17.791259: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-07 11:26:17.791364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-07 11:26:17.931720: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers\n",
        "import tensorflow as tf\n",
        "# Function to load and preprocess images\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize images as per your network requirements\n",
        "    img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7fY-992fpLh"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:12.099168Z",
          "iopub.status.busy": "2024-02-07T11:27:12.09802Z",
          "iopub.status.idle": "2024-02-07T11:27:12.103319Z",
          "shell.execute_reply": "2024-02-07T11:27:12.102346Z",
          "shell.execute_reply.started": "2024-02-07T11:27:12.099134Z"
        },
        "id": "n_djBRvqXpVV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Paths of the two folders mentioned in the beginning\n",
        "normal_folder = \"/kaggle/working/Text 3 images/Text 3 - Training images/Pass\"\n",
        "misprint_folder = \"/kaggle/working/Text 3 images/Text 3 - Training images/Missing Text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:12.333616Z",
          "iopub.status.busy": "2024-02-07T11:27:12.333003Z",
          "iopub.status.idle": "2024-02-07T11:27:12.338568Z",
          "shell.execute_reply": "2024-02-07T11:27:12.337585Z",
          "shell.execute_reply.started": "2024-02-07T11:27:12.333583Z"
        },
        "id": "D8DvclbRXpVW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Gets paths of all files in a directory\n",
        "def get_image_paths(directory):\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(directory):\n",
        "            image_paths.append(os.path.join(directory, filename))\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:13.708459Z",
          "iopub.status.busy": "2024-02-07T11:27:13.708088Z",
          "iopub.status.idle": "2024-02-07T11:27:13.714542Z",
          "shell.execute_reply": "2024-02-07T11:27:13.713737Z",
          "shell.execute_reply.started": "2024-02-07T11:27:13.708431Z"
        },
        "id": "_Xh_OMPuXpVW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Getting paths of all the normal and misprinted images in two separate lists\n",
        "normal_images_paths = get_image_paths(normal_folder)\n",
        "misprinted_images_paths = get_image_paths(misprint_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdTTgl8rfpLj"
      },
      "source": [
        "## Splitting data paths into paths for train, test and val sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:15.454036Z",
          "iopub.status.busy": "2024-02-07T11:27:15.45324Z",
          "iopub.status.idle": "2024-02-07T11:27:15.462235Z",
          "shell.execute_reply": "2024-02-07T11:27:15.461175Z",
          "shell.execute_reply.started": "2024-02-07T11:27:15.454001Z"
        },
        "id": "KXHiCjISXpVW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def split_paths(paths, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Split a list of paths into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "    - paths (list): List of paths to be split.\n",
        "    - train_ratio (float): Ratio of training set size (default: 0.7).\n",
        "    - val_ratio (float): Ratio of validation set size (default: 0.15).\n",
        "    - seed (int): Seed for reproducibility (default: None).\n",
        "\n",
        "    Returns:\n",
        "    - train_paths (list): List of paths for the training set.\n",
        "    - val_paths (list): List of paths for the validation set.\n",
        "    - test_paths (list): List of paths for the test set.\n",
        "    \"\"\"\n",
        "    # Set seed for reproducibility if provided\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "\n",
        "    # Shuffle the list of paths\n",
        "    random.shuffle(paths)\n",
        "\n",
        "    # Calculate sizes for train, validation, and test sets\n",
        "    total_size = len(paths)\n",
        "    train_size = int(train_ratio * total_size)\n",
        "    val_size = int(val_ratio * total_size)\n",
        "\n",
        "    # Split the shuffled list into train, validation, and test sets\n",
        "    train_paths = paths[:train_size]\n",
        "    val_paths = paths[train_size:train_size + val_size]\n",
        "    test_paths = paths[train_size + val_size:]\n",
        "\n",
        "    # Ensure test_paths gets the remaining items if sizes don't sum up to total_size\n",
        "    if len(test_paths) + len(val_paths) + len(train_paths) != total_size:\n",
        "        test_paths += paths[train_size + val_size + len(test_paths):]\n",
        "\n",
        "    return train_paths, val_paths, test_paths\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:15.993655Z",
          "iopub.status.busy": "2024-02-07T11:27:15.993266Z",
          "iopub.status.idle": "2024-02-07T11:27:15.999083Z",
          "shell.execute_reply": "2024-02-07T11:27:15.998093Z",
          "shell.execute_reply.started": "2024-02-07T11:27:15.993623Z"
        },
        "id": "cB4vRgpTXpVX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Getting the split paths\n",
        "train_normal_paths, val_normal_paths, test_normal_paths = split_paths(normal_images_paths)\n",
        "train_misprinted_paths, val_misprinted_paths, test_misprinted_paths = split_paths(misprinted_images_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CkkacJHfpLk"
      },
      "source": [
        "## Triplets generator function\n",
        "### This function generates triplets for triplet loss training. It takes lists of file paths for normal and misprinted images, and the number of triplets to generate as input arguments. It randomly selects an anchor image from one of the classes ('Pass' or 'Missing Text'), then selects a positive image from the same class and a negative image from the other class. Finally, it loads these images and returns them as triplets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:17.490824Z",
          "iopub.status.busy": "2024-02-07T11:27:17.490039Z",
          "iopub.status.idle": "2024-02-07T11:27:17.500157Z",
          "shell.execute_reply": "2024-02-07T11:27:17.499074Z",
          "shell.execute_reply.started": "2024-02-07T11:27:17.490793Z"
        },
        "id": "-mSGbvpLget3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to generate triplets\n",
        "def generate_triplets(normal_image_paths,misprinted_images_paths, num_triplets):\n",
        "    triplet_pairs = []\n",
        "    classes = ['Pass','Missing Text']\n",
        "\n",
        "\n",
        "    for _ in range(num_triplets):\n",
        "        # Select a random class for anchor\n",
        "        anchor_class = np.random.choice(classes)\n",
        "\n",
        "        if(anchor_class == 'Pass'):\n",
        "          # Select a random anchoro from Pass\n",
        "          anchor_path = np.random.choice(normal_image_paths)\n",
        "          # Select a positive image from the pass\n",
        "          positive_image_path = np.random.choice(normal_image_paths)\n",
        "          # Make sure anchor and positive image are not the same\n",
        "          while (anchor_path == positive_image_path):\n",
        "                positive_image_path = np.random.choice(normal_image_paths)\n",
        "          # Select a positive image from the Missing Text\n",
        "          negative_image_path = np.random.choice(misprinted_images_paths)\n",
        "          # load images\n",
        "          anchor = load_image(anchor_path)\n",
        "          positive = load_image(positive_image_path)\n",
        "          negative = load_image(negative_image_path)\n",
        "\n",
        "        elif (anchor_class == 'Missing Text'):\n",
        "          # select a random anchor from Missing Text\n",
        "          anchor_path = np.random.choice(misprinted_images_paths)\n",
        "          # Select a random positive image from Missing Text\n",
        "          positive_image_path = np.random.choice(misprinted_images_paths)\n",
        "          # Make sure anchor and positive image are not the same\n",
        "          while (anchor_path == positive_image_path):\n",
        "                positive_image_path = np.random.choice(misprinted_images_paths)\n",
        "          # Select a random negative image from Pass\n",
        "          negative_image_path = np.random.choice(normal_image_paths)\n",
        "\n",
        "          # Load images\n",
        "          anchor = load_image(anchor_path)\n",
        "          positive = load_image(positive_image_path)\n",
        "          negative = load_image(negative_image_path)\n",
        "\n",
        "\n",
        "        triplet_pairs.append((anchor, positive, negative))\n",
        "\n",
        "    return np.array(triplet_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNia545xfpLl"
      },
      "source": [
        "# Loading a pre-trained mobilenet without the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:21.154404Z",
          "iopub.status.busy": "2024-02-07T11:27:21.153413Z",
          "iopub.status.idle": "2024-02-07T11:27:24.455632Z",
          "shell.execute_reply": "2024-02-07T11:27:24.454765Z",
          "shell.execute_reply.started": "2024-02-07T11:27:21.154368Z"
        },
        "id": "c2z_YH0pngbx",
        "outputId": "b4d63526-8074-4aec-9f7d-e5e13e309249",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications import MobileNetV2\n",
        "from keras import layers\n",
        "target_shape = (224,224)\n",
        "model = MobileNetV2(\n",
        "    input_shape=(224,224,3),\n",
        "    alpha=1.0,\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation=\"softmax\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:24.457726Z",
          "iopub.status.busy": "2024-02-07T11:27:24.457393Z",
          "iopub.status.idle": "2024-02-07T11:27:24.483266Z",
          "shell.execute_reply": "2024-02-07T11:27:24.482331Z",
          "shell.execute_reply.started": "2024-02-07T11:27:24.457676Z"
        },
        "id": "ggLWEsffqcRx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Adding some convolutional layers to it to increase the final output to more than 1000 units\n",
        "last_layer = model.layers[-1].output\n",
        "out0 =layers.Conv2D(filters=24, kernel_size=(1, 1), padding='same')(last_layer)\n",
        "out =layers.Flatten()(out0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:24.484901Z",
          "iopub.status.busy": "2024-02-07T11:27:24.484515Z",
          "iopub.status.idle": "2024-02-07T11:27:24.712142Z",
          "shell.execute_reply": "2024-02-07T11:27:24.711107Z",
          "shell.execute_reply.started": "2024-02-07T11:27:24.484867Z"
        },
        "id": "hhNcZ2btoM5u",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fin =  Model(inputs=model.input, outputs=out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:21:36.023588Z",
          "iopub.status.busy": "2024-02-07T10:21:36.023165Z",
          "iopub.status.idle": "2024-02-07T10:21:36.035133Z",
          "shell.execute_reply": "2024-02-07T10:21:36.033947Z",
          "shell.execute_reply.started": "2024-02-07T10:21:36.023561Z"
        },
        "id": "3Il_uXploM9J",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Only keeping  the last 5 layers trainable\n",
        "for layer in fin.layers[:-5]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:21:37.213125Z",
          "iopub.status.busy": "2024-02-07T10:21:37.212185Z",
          "iopub.status.idle": "2024-02-07T10:21:37.21848Z",
          "shell.execute_reply": "2024-02-07T10:21:37.217383Z",
          "shell.execute_reply.started": "2024-02-07T10:21:37.213069Z"
        },
        "id": "kIKPCPpiXpVa",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#fin.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yakoxSzNfpLn"
      },
      "source": [
        "### Triplet loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:25.727278Z",
          "iopub.status.busy": "2024-02-07T11:27:25.726904Z",
          "iopub.status.idle": "2024-02-07T11:27:25.733586Z",
          "shell.execute_reply": "2024-02-07T11:27:25.732534Z",
          "shell.execute_reply.started": "2024-02-07T11:27:25.727248Z"
        },
        "id": "ava-Lh-1gev9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def triplet_loss(anchor, positive, negative, alpha=0.5):\n",
        "    pos_dist = K.sum(K.square(anchor - positive), axis=-1)\n",
        "    neg_dist = K.sum(K.square(anchor - negative), axis=-1)\n",
        "    basic_loss = pos_dist - neg_dist + alpha\n",
        "    loss = K.maximum(basic_loss, 0.0)\n",
        "\n",
        "    # Take the mean or sum across the batch dimension\n",
        "    loss = K.mean(loss)  # or K.sum(loss)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-cIuBMfpLo"
      },
      "source": [
        "## Training Loop Description\n",
        "\n",
        "The training loop iterates over the specified number of epochs. Within each epoch:\n",
        "\n",
        "1. **Data Generation:**\n",
        "   - Generate triplets for training using `generate_triplets()` function with normal and misprinted image paths.\n",
        "   - Generate validation triplets using a smaller number of samples for validation.\n",
        "   \n",
        "2. **Forward Pass:**\n",
        "   - Perform a forward pass through the Siamese network (denoted as `fin`) with the anchor, positive, and negative images to obtain embeddings.\n",
        "   \n",
        "3. **Triplet Loss Calculation:**\n",
        "   - Calculate the triplet loss using the obtained anchor, positive, and negative embeddings.\n",
        "   \n",
        "4. **Backward Pass:**\n",
        "   - Compute gradients of the loss with respect to trainable variables using `tf.GradientTape`.\n",
        "   - Update model weights using the optimizer (`optimizer`) based on the computed gradients.\n",
        "   \n",
        "5. **Validation:**\n",
        "   - Obtain embeddings for validation triplets.\n",
        "   - Calculate validation triplet loss.\n",
        "   - Save the weights if the current validation loss is the best seen so far (`best_val_loss`).\n",
        "   \n",
        "6. **Monitoring and Saving:**\n",
        "   - Append training and validation losses to their respective lists.\n",
        "   - Print the training and validation losses for monitoring purposes.\n",
        "   - Save the weights periodically (`current_siamese.h5`) and if validation loss improves (`best_siamese.h5`).\n",
        "   \n",
        "7. **Resource Management:**\n",
        "   - Delete the tape to free up resources after each iteration.\n",
        "   \n",
        "8. **Completion:**\n",
        "   - After all epochs are completed, print \"Training complete.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr44915eApYh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "epochs = 100\n",
        "best_val_loss = 2000\n",
        "num_triplets = 50\n",
        "train_losses= []\n",
        "val_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4WBQfv_1ZP7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    triplets = generate_triplets(train_normal_paths,train_misprinted_paths, num_triplets)\n",
        "\n",
        "    val_triplets = generate_triplets(val_normal_paths,val_misprinted_paths, 20)\n",
        "    anchors = triplets[:, 0]\n",
        "    positives = triplets[:, 1]\n",
        "    negatives = triplets[:, 2]\n",
        "\n",
        "    # Forward pass to get embeddings\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        anchor_embeddings = fin(anchors)\n",
        "        positive_embeddings = fin(positives)\n",
        "        negative_embeddings = fin(negatives)\n",
        "\n",
        "        # Calculate triplet loss\n",
        "        loss_value = triplet_loss(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "\n",
        "    # Backward pass\n",
        "    gradients = tape.gradient(loss_value, fin.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, fin.trainable_variables))\n",
        "\n",
        "    # Print loss for monitoring\n",
        "\n",
        "\n",
        "    val_anchors = val_triplets[:, 0]\n",
        "    val_positives = val_triplets[:, 1]\n",
        "    val_negatives = val_triplets[:, 2]\n",
        "    val_anchor_embeddings = fin.predict(val_anchors)\n",
        "    val_positive_embeddings = fin.predict(val_positives)\n",
        "    val_negative_embeddings = fin.predict(val_negatives)\n",
        "    val_loss_value = triplet_loss(val_anchor_embeddings, val_positive_embeddings, val_negative_embeddings)\n",
        "\n",
        "    if(val_loss_value.numpy() <= best_val_loss):\n",
        "      fin.save_weights(\"best_siamese.h5\")\n",
        "    train_losses.append(loss_value.numpy())\n",
        "    val_losses.append(val_loss_value.numpy())\n",
        "    if (epoch%10 ==0):\n",
        "        print(f\"Epoch {epoch + 1}, Train Loss: {loss_value.numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1}, Val Loss: {val_loss_value.numpy()}\")\n",
        "        fin.save_weights(\"current_siamese.h5\")\n",
        "\n",
        "    # Delete the tape to free up resources\n",
        "    del tape\n",
        "\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuCYrBODfpLp"
      },
      "source": [
        "# Plotting loss curves.\n",
        "(I lost the plot accidentally. You can retrain the model and plot if necessary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am_qBsT9XpVc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyO6f1zhfpLq"
      },
      "source": [
        "# Loading the best saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:35.184912Z",
          "iopub.status.busy": "2024-02-07T11:27:35.184226Z",
          "iopub.status.idle": "2024-02-07T11:27:35.583853Z",
          "shell.execute_reply": "2024-02-07T11:27:35.582845Z",
          "shell.execute_reply.started": "2024-02-07T11:27:35.184879Z"
        },
        "id": "nOCTf0tZ_S3w",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "weights_path = '/kaggle/input/best-model/best_siamese.h5'\n",
        "\n",
        "# Load the weights into the model\n",
        "fin.load_weights(weights_path,by_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na9u1jQ0fpLq"
      },
      "source": [
        "### The next function uses the best loaded model and compares a test image with one misprinted and normal image and returns 1 if the distance of test image from misprinted image in less than that from the normal image. Otherwise, it returns zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:22:44.036976Z",
          "iopub.status.busy": "2024-02-07T10:22:44.036028Z",
          "iopub.status.idle": "2024-02-07T10:22:44.044451Z",
          "shell.execute_reply": "2024-02-07T10:22:44.043156Z",
          "shell.execute_reply.started": "2024-02-07T10:22:44.036941Z"
        },
        "id": "fYGiNyPjXpVd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def sub_pred(test_image):\n",
        "    pos = load_image(np.random.choice(train_misprinted_paths))\n",
        "    neg = load_image(np.random.choice(train_normal_paths))\n",
        "\n",
        "    anchor_embeddings = fin(tf.expand_dims(test_image,axis=0))\n",
        "    positive_embeddings = fin(tf.expand_dims(pos,axis=0))\n",
        "    negative_embeddings = fin(tf.expand_dims(neg,axis=0))\n",
        "    pos_dist = K.sum(K.square(anchor_embeddings - positive_embeddings), axis=-1).numpy()[0]\n",
        "    neg_dist = K.sum(K.square(anchor_embeddings - negative_embeddings), axis=-1).numpy()[0]\n",
        "    if (pos_dist < neg_dist):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHaqNyawfpLr"
      },
      "source": [
        "### Below is a function that performs prediction on all of the training set.\n",
        "#### For each entry, it performs the majority voting of 7 predictions given by the function above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:33:04.112097Z",
          "iopub.status.busy": "2024-02-07T10:33:04.111298Z",
          "iopub.status.idle": "2024-02-07T10:33:04.120877Z",
          "shell.execute_reply": "2024-02-07T10:33:04.119796Z",
          "shell.execute_reply.started": "2024-02-07T10:33:04.112045Z"
        },
        "id": "c6j1JgJQ7WWu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def metric_calculator():\n",
        "    mis_print_preds = []\n",
        "    for path in test_misprinted_paths:\n",
        "        test_image = load_image(path)\n",
        "        pred1 = sub_pred(test_image)\n",
        "        pred2 = sub_pred(test_image)\n",
        "        pred3 = sub_pred(test_image)\n",
        "        pred4 = sub_pred(test_image)\n",
        "        pred5 = sub_pred(test_image)\n",
        "        pred6 = sub_pred(test_image)\n",
        "        pred7 = sub_pred(test_image)\n",
        "\n",
        "        mis_print_preds.append((pred1 + pred2 + pred3+pred4 + pred5 + pred6+pred7)//4)\n",
        "    normal_preds = []\n",
        "    for path in test_normal_paths:\n",
        "        test_image = load_image(path)\n",
        "        pred1 = sub_pred(test_image)\n",
        "        pred2 = sub_pred(test_image)\n",
        "        pred3 = sub_pred(test_image)\n",
        "        pred4 = sub_pred(test_image)\n",
        "        pred5 = sub_pred(test_image)\n",
        "        pred6 = sub_pred(test_image)\n",
        "        pred7 = sub_pred(test_image)\n",
        "\n",
        "\n",
        "        normal_preds.append((pred1 + pred2 + pred3+pred4 + pred5 + pred6+pred7)//4)\n",
        "    return mis_print_preds,normal_preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J974emw5fpLr"
      },
      "source": [
        "### Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:33:04.483802Z",
          "iopub.status.busy": "2024-02-07T10:33:04.483407Z",
          "iopub.status.idle": "2024-02-07T10:35:51.523203Z",
          "shell.execute_reply": "2024-02-07T10:35:51.522214Z",
          "shell.execute_reply.started": "2024-02-07T10:33:04.483771Z"
        },
        "id": "dQA9ggMR7YwF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "mis_print_preds,normal_preds = metric_calculator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csYgHnOGfpLs"
      },
      "source": [
        "### Combining predictions and making ground truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T10:36:12.164709Z",
          "iopub.status.busy": "2024-02-07T10:36:12.164285Z",
          "iopub.status.idle": "2024-02-07T10:36:12.170501Z",
          "shell.execute_reply": "2024-02-07T10:36:12.169292Z",
          "shell.execute_reply.started": "2024-02-07T10:36:12.164676Z"
        },
        "id": "5e_Ltc3-8Fj2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "mis_print_preds = np.asarray(mis_print_preds)\n",
        "ground_truth_mis_print = np.ones_like(mis_print_preds)\n",
        "normal_preds = np.asarray(normal_preds)\n",
        "ground_truth_normal = np.zeros_like(normal_preds)\n",
        "\n",
        "preds = np.zeros(len(mis_print_preds)+len(normal_preds))\n",
        "preds[0:len(mis_print_preds)] = (mis_print_preds)\n",
        "preds[len(mis_print_preds) : ] = normal_preds\n",
        "\n",
        "gt = np.zeros(len(mis_print_preds)+len(normal_preds))\n",
        "gt[0:len(mis_print_preds)] =  ground_truth_mis_print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY92DMyDfpLy"
      },
      "source": [
        "# Accuracy on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-02-07T10:36:13.594472Z",
          "iopub.status.busy": "2024-02-07T10:36:13.594109Z",
          "iopub.status.idle": "2024-02-07T10:36:13.601877Z",
          "shell.execute_reply": "2024-02-07T10:36:13.600634Z",
          "shell.execute_reply.started": "2024-02-07T10:36:13.594446Z"
        },
        "id": "ixQM6ozU8JoQ",
        "outputId": "2144ffab-d550-40c8-ccda-60da0eb08dc0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total accuracy: 0.8846153846153846\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "total_accuracy = accuracy_score(preds, gt)\n",
        "\n",
        "print(\"Total accuracy:\", total_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-02-07T11:27:48.721616Z",
          "iopub.status.busy": "2024-02-07T11:27:48.721245Z",
          "iopub.status.idle": "2024-02-07T11:27:49.037971Z",
          "shell.execute_reply": "2024-02-07T11:27:49.036927Z",
          "shell.execute_reply.started": "2024-02-07T11:27:48.721587Z"
        },
        "id": "7haDO3N4fpLz",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "b689d7c0-7ff2-44a8-f5ab-b844509052d9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Saving the whole  model\n",
        "fin.save(\"model.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook357bc11d9d",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4412465,
          "sourceId": 7580074,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
